{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeb2c963-71c1-4110-acd6-90010427e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_potential(A, b, L):\n",
    "\n",
    "    # chapter 2.2.7.5 in Murphy, Probabilistic Machine Learning: Advanced Topics\n",
    "    # potential over (x_{t - 1}, x_t) or (x_t, y_t), in that order\n",
    "\n",
    "    g = - 0.5 * (b @ L @ b + b.size * np.log(2 * math.pi) + np.log(np.linalg.det(np.linalg.inv(L)))) # is b.size correct dim?\n",
    "    K = np.block([[A.T @ L @ A, - A.T @ L], [- L @ A, L]]) # is A transpose the right way?\n",
    "    h = np.concatenate((- A.T @ L @ b, L @ b))\n",
    "\n",
    "    return g, K, h\n",
    "\n",
    "def condition_potential_on_evidence(g, K, h, y):\n",
    "\n",
    "    # chapter 2.2.7.4 in Murphy, Probabilistic Machine Learning: Advanced Topics\n",
    "\n",
    "    g_new = g + h[-y.size:] @ y - 0.5 * y @ K[-y.size:, -y.size:] @ y\n",
    "    K_new = K[:-y.size, :-y.size]\n",
    "    h_new = h[:-y.size] - K[:-y.size, -y.size:] @ y\n",
    "\n",
    "    return g_new, K_new, h_new\n",
    "\n",
    "\n",
    "def log_normaliser(J, h, diagonal_boost = 1e-9):\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Exponential_family\n",
    "\n",
    "    # https://math.stackexchange.com/questions/3158303/using-cholesky-decomposition-to-compute-covariance-matrix-determinant\n",
    "    L = np.linalg.cholesky(J + diagonal_boost * np.eye(J.shape[-1]))\n",
    "    half_log_det_precision = np.log(np.diagonal(L, axis1 = -2, axis2 = -1)).sum(-1)\n",
    "\n",
    "    return 0.5 * h @ np.linalg.solve(J, h) - half_log_det_precision\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "C = np.random.randn(3,3)\n",
    "d = np.random.randn(3,)\n",
    "R = np.random.randn(3,3)\n",
    "R = R @ R.T * 10\n",
    "y = np.random.randn(3,)\n",
    "x = np.random.randn(3,)\n",
    "\n",
    "gE, KE, hE = edge_potential(C, d, np.linalg.inv(R))\n",
    "gC, KC, hC = condition_potential_on_evidence(gE, KE, hE, y)\n",
    "Sig = np.linalg.inv(KC)\n",
    "mu = Sig @ hC\n",
    "\n",
    "# this is a probability distribution wrt y, so this computation is valid\n",
    "noise = tfd.MultivariateNormalFullCovariance(loc=d, covariance_matrix=R)\n",
    "obs_ll_from_likelihood = noise.log_prob(y - C @ x)\n",
    "\n",
    "# obs_ll_from_likelihood can also be computed as follows:\n",
    "# here we evaluate the potential directly (Murph 2, equation 2.112)\n",
    "obs_ll_from_likelihood_version2 = gC + x @ hC - 0.5 * x @ KC @ x\n",
    "\n",
    "# this is not a probability distribution wrt x, it is an unormalised potential on x, \n",
    "# so this is not a valid computation as MVN assumes that this is a probability distribution wrt x\n",
    "noise = tfd.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=Sig)\n",
    "obs_ll_from_potential = noise.log_prob(x)\n",
    "\n",
    "# we can compute obs_ll_from_likelihood from obs_ll_from_potential as follows:\n",
    "# here we subtract the incorrectly assumed scaling factor (the inverse of the normalising constant for the potential) \n",
    "# and then add the true scaling factor gC\n",
    "assumed_scaling = (np.log((2 * math.pi) **(-3/2)) - log_normaliser(KC, hC))\n",
    "corrected_obs_ll_from_potential = obs_ll_from_potential - assumed_scaling + gC\n",
    "\n",
    "# NB log_normaliser(KC, hC) = -(np.log(np.linalg.det(Sig)**(-1/2)) -0.5 * mu @ KC @ mu)\n",
    "# log_normaliser for a Gaussian is not the full normalising constant\n",
    "# it does include not base measure (the 2 * pi term), which we add in to get assumed_scaling\n",
    "\n",
    "# because obs_ll_from_potential is not equal to obs_ll_from_likelihood,\n",
    "# the true marginal likelihood is not equal to the one which assumes the potentials on x are pdfs on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1df1c33d-8c96-49ec-854d-8a53311b7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.9088416\n",
      "-5.908841493889845\n",
      "-5.908842\n",
      "-8.008678\n"
     ]
    }
   ],
   "source": [
    "print(obs_ll_from_likelihood)\n",
    "print(obs_ll_from_likelihood_version2)\n",
    "print(corrected_obs_ll_from_potential)\n",
    "print(obs_ll_from_potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483af5c0-1fe0-4798-a93c-47e503499fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior.log_normalizer in svae is not the same as the marginal likelihood, lps, computed\n",
    "# using the actual likelihood (for the reasons above), but the correction can be computed:\n",
    "\n",
    "lps - (posterior.log_normalizer + ll_correction.sum())\n",
    "\n",
    "def edge_potential(self, A, b, L):\n",
    "\n",
    "    # chapter 2.2.7.5 in Murphy, Probabilistic Machine Learning: Advanced Topics\n",
    "    # potential over (x_{t - 1}, x_t) or (x_t, y_t), in that order\n",
    "\n",
    "    g = - 0.5 * (b @ L @ b + b.size * np.log(2 * math.pi) + np.log(np.linalg.det(np.linalg.inv(L)))) # is b.size correct dim?\n",
    "    K = np.block([[A.T @ L @ A, - A.T @ L], [- L @ A, L]]) # is A transpose the right way?\n",
    "    h = np.concatenate((- A.T @ L @ b, L @ b))\n",
    "\n",
    "    return g, K, h\n",
    "\n",
    "def condition_potential_on_evidence(self, g, K, h, y):\n",
    "\n",
    "    # chapter 2.2.7.4 in Murphy, Probabilistic Machine Learning: Advanced Topics\n",
    "\n",
    "    g_new = g + h[-y.size:] @ y - 0.5 * y @ K[-y.size:, -y.size:] @ y\n",
    "    K_new = K[:-y.size, :-y.size]\n",
    "    h_new = h[:-y.size] - K[:-y.size, -y.size:] @ y\n",
    "\n",
    "    return g_new, K_new, h_new\n",
    "\n",
    "def log_normaliser(self, J, h, diagonal_boost = 1e-9):\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Exponential_family\n",
    "\n",
    "    # https://math.stackexchange.com/questions/3158303/using-cholesky-decomposition-to-compute-covariance-matrix-determinant\n",
    "    L = np.linalg.cholesky(J + diagonal_boost * np.eye(J.shape[-1]))\n",
    "    half_log_det_precision = np.log(np.diagonal(L, axis1 = -2, axis2 = -1)).sum(-1)\n",
    "\n",
    "    return 0.5 * h @ np.linalg.solve(J, h) - half_log_det_precision\n",
    "\n",
    "def correct_ll(self, C, d, R, y):\n",
    "\n",
    "    gE, KE, hE = self.edge_potential(C, d, np.linalg.inv(R))\n",
    "    gC, KC, hC = self.condition_potential_on_evidence(gE, KE, hE, y)\n",
    "    ll_correction = gC - (np.log((2 * np.pi) **(-3/2)) - self.log_normaliser(KC, hC))\n",
    "\n",
    "    return ll_correction\n",
    "\n",
    "ll_correction = vmap(self.correct_ll, in_axes=(None,None,None,0))(params[\"C\"], params[\"d\"], params[\"R\"], data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
